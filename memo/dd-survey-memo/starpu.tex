\section{StarPU}

\subsection{Overview}

StarPU is developed at LaBRI, Inria Bordeaux Univercity. 
StarPU is a task-programming library for hybrid architectures. It features a runtime system for executing application on heterogeneous machines, i.e. with GPUs, CPUs. 
The abstraction for a task is the \emph{codelet}. The codelet is a set of implementations of the same computational kernel for different computation units (i.e. CPU, GPU). 
The programmer specifies the direction of the codelets parameters so that the runtime discovers and enforces the dependencies between them.
Codelets are then executed asunchronously on a core of the machine or offloaded to an accelerator. The StarPU runtime targets shared memory heterogeneous platforms, and a set of futures are provided:
The StarPU runtime does not work directly on distributed memory machines, instead they
propose an hybrid model that integrates with MPI through explicit network communications, which will then be automatically combined and overlapped with the intra-node data transfers and computation.

Related to scalability:
Alternatively, the application can also just provide the whole task graph, a data distribution over MPI nodes, and StarPU will automatically determine which MPI node should execute which task, 
and generate all required MPI communications accordingly (new in v0.9). We have gotten excellent scaling on a 144-node cluster with GPUs, we have not yet had the opportunity to test on a yet larger cluster. 
We have however measured that with naive task submission, it should scale to a thousand nodes, and with pruning-tuned task submission, it should scale to about a million nodes. 
TODO: learn more about this, does this mean that the data-depencency analysis is not done authomatically then? how do you provide the whole task graph (in which format)?

The design of StarPU is organized around three main componenets: (i) the codelets; (ii) A data management library: responsible of maintainin data coherency and perform data management (i.e. prefetching, reordering, asynchronoums memory transfers ..); And (iii) A scheduling framework: StarPU exposes a  framework to implement scheduling policies and various scheduling policies are implemented.

One interesting feature of StarPU is their work on scheduling strategies based on performance models. They propose, rather than building a performance model by hand (i.e. using the ration between the number of operations and the speed of the processor for every kernel and computational unit), building a history-based performance model dynamically either by calibrating parametric models by means of regressions or by storing the performance of tasks on different inputs, identifying task kinds and using this information to predict/estimate performance of each kernel in the the different cu for different input sizes. This history-based performance model relies on some regularity hypothesis and also tasks' execution time should be independent from the actual content of the data. The latter is not the case for SDP visibility data.


\subsection{Requirements overview}

\begin{itemize}
\item Data Layout. The programming model runs on a shared memory machine, data can be distributed over nodes by using MPI and then an hybrid model is used TODO:learn more about this. MPI 2.0 does not easily support irregular data structures. TODO: check MPI3.0 and learn more about this How to distribute data in chunks of different sizes with MPI.
\item Memory management: When memory is not big enough for the working set StarPU provides a mechanism (out-or-core support) that allows the runtime to automatically evict data from the main memory to disk in advance, and prefetch back required data before it is needed for tasks . The principle is that the program/task first registers a disk location, seen by StarPU as a void*, which can be for instance a Unix path for the stdio or unistd case, or a database file path for a leveldb case, etc. The disk backend opens this place with the plug method.
If the disk backend provides an alloc method, StarPU can then start using it to allocate room and store data there with the write method, without user intervention. 
This also means that Access to storage (e.g., in our case the buffered visibilities, intermediate data products, final data products) can be intermediated by StarPU.
\item Scalability: They have proven scaling on a 144-node cluster with GPUs (of the hybrid version with MPI)
\item Interoperability: They have compilers that transpate from OpenAcc and OpenMP to StarPU code. They interoperate with MPI to provide an hybrid model that can run on clusters. 
The codelets (task-abstraction) are a set of implementations of the same kernel for different architectures (i.e. GPU,CPU) and even parallel (OpenMP). TODO:find out more!
\item I/O: The storage service can be abstracted by the out-of-core support mentioned above. Not suer about the streaming.
\item performance: Performance is TODO { the overhead added by the runtime should be acceptable, performance comparable to an MPI program (ASKAPSoft). There is a timeout: the program should run before the next observation is available (6 hours). Factors that affect the overhead of a runtime are: communication overhead (we can measure latency and bandwidth between two nodes and bi-sectional bandwidth using the full 8k nodes), runtime overhead (when communication is not an issue, we would like to measure performance of an embarassingly parallel program which should be comparable to SPMD program (i.e. MPI). Data transposition: A two-dimensional region will be used row-wise by a certain number of tasks but it will need to be used colum-wise by a different tasks, so, effectively it will require a transposition of the data, we would like to measure how the runtime can handle this (without explicitely using collective operations)} 
\item resilience: Resilience is not mentioned. 
\item Maintainability/ Extensibility: In the case of StarPU, although kernels are kept separated by a well defined interface it has the drawback that it does not support clusters directly but it offers some services to work together with MPI, the application data partitioning and communication are handled explicitely by the programmer. This hinders productibity and extensibility and maintainability, because it means i.e. in case of a hardware upgrade, where more nodes are added, the program would need to be modified to partition appropiately.
\item Performance Portability/ Extensibility: same as above.
\end{itemize}
