\section{Legion/Regent}

\subsection{Overview}

Legion is a data-centric programming system for writing portable high performance programs targeted at distributed heterogeneous architectures.
Legion presents abstractions which allow programmers to describe properties of program data (e.g. independence, locality). The Regent language provides a convenient way to program against the Legion runtime. Regent and Legion are active research projects at Stanford University.	

The legion programming model provides three important abstractions: 
\begin{enumerate}
\item Logical Regions: Logical regions are the fundamental abstraction used for describing program data in Legion applications. Logical regions are regarded as abstract data without considering its location in a particular memory system and without a fixed layout in memory. Each logical region is described by an index space of rows (or keys) (either unstructured pointers or structured 1D, 2D, or 3D arrays) and a field space of columns (or values). Regions can be recursively partitioned to match the hierarchical structure of memory and to facilitate parallel execution on subsets of data. Regions can also be partitioned multiple times to have different views of the data.  
%Regions support a relational model for data. 
% Data structures can be encoded in logical regions to express locality with partitioning and slicing describing data independence.
\item Tasks: A Regent program looks like sequential program with calls to tasks, which are functions that the programmer has marked as eligible for parallel execution. Regent guarantees sequential consistency. A program executes as a tree of tasks with a top-level task spawning sub-tasks which can recursively spawn further sub-tasks. All tasks in Legion must specify the logical regions they will access as well as the privileges (read, write, reduce) and coherence (only in Legion: exclusive, atomic, simultaneous or relaxed) for each logical region.
\item Mapping Interface: Legion makes no implicit decisions concerning how applications are mapped onto target hardware. Instead mapping decisions regarding how tasks are assigned to processors and how physical instances of logical regions are assigned to memories are made entirely by mappers. Mappers are part of application code and implement a mapping interface. Mappers are queried by the Legion runtime whenever any mapping decision needs to be made. Mappers are not exposed to Regent language.
\end{enumerate}

\subsection{Requirements overview}

Taking into account the list of requirements that have been identified for the SDP runtime we learned the following: 
\begin{itemize}
\item Data Layout. Visibility data should be irregularly distributed to balance computation. In Regent, Regions describe how data is used by the program. Regions can be arbitrarily partitioned into sub-regions based on index space or sliced on their field space. They can express partitions for handling irregularly sampled collections. Regions are not mapped to any physical memory position, this is done by the mapper which can only be expressed in Legion.
\item Memory management: Another challenge related to data occurs when the amount of data needed by a task exceeds the physical available memory. 
Regent allows the use of files (i.e in HDF5 format) that can be mapped to regions and used as such in the data dependency analysis, however the program should be aware that data would not fit in memory and use files instead. If programmer is not careful I do not know how would regent handle //TODO a test!! Also related with memory management there will be the situation when the memory requirements of two tasks exceeds the amount of available physical memory. In Regent tasks specify the regions they use and their permissions for those regions (weather the task performs reads, writes or reductions to each region). The information of how much data is needed by a specific task is known by the runtime and the scheduling decision happens at the mapper, however  mappers do not have view of the global scheduling state, i.e. I could not figure out how to see the available/free memory in a node so that this can be taken into account when scheduling tasks.   
\item Scalability: We seek to achieve scalability up to 8K cores. Legion has proven scalability up to 10K~\cite{MBauerPhD}, however Regent language scalability is limited to a small number of nodes (between 10 and 100)~\cite{Regent}. While Legion allows simultaneous access to regions (providing relaxed coherence ?). In regent each task has exclusive access to its region arguments, which limits scalability.
In Legion each task specifies for each region it accesses the privileges and coherence it requires of those regions. Privileges specify what the function can do with the regions while coherence specifies what other functions can do with the regions concurrently. Regent assumes all accesses to regions are exclusive which limits parallelism.
\item Interoperability: Regent provides interoperability with OpenMP,  MPI, CUDA, OpenACC and foreign functions  (i.e fft library). However when there is no way at the moment to request specific resources for a task to run (i.e number of cores, number of nodes...) which limits somehow the usefulness of this interoperability.
\item I/O: a way to get data into the system (visibility data comes from the TM, streaming, and then from buffer, other data i.e. sky model, telescope state is in the storage service (file?). Nothing prevents a task to read from a socket (I think ...), or a file (files can even be mapped to regions) to take into account data dependencies but that would not be necessary (talk to Danielle about how did they implement streaming in COMPSS)
\item performance: One factor that affect the overhead of a runtime is communication overhead. We have measured latency and bandwidth between two nodes and results are not very satisfying. Measurements in Darwin show that latency is around 5.5us. while the underlying communication system (GASNET MXM) achieves 2us latency. Regarding the bandwidth the theoretical peak in Darwin is around 7GB/s is fully utilized by GASNET MXM , from the Regent code we measured around 280MB/s. bi-sectional bandwidth using the full 8k nodes (TODO). runtime overhead (when communication is not an issue, we would like to measure performance of an embarassingly parallel program which should be comparable to SPMD program (i.e. MPI)(TODO). Data transposition: A two-dimensional region will be used row-wise by a certain number of tasks but it will need to be used colum-wise by a different tasks, so, efectively it will require a transposition of the data, we would like to measure how the runtime can handle this (without explicitely using collective operations). We need a better benchmark to mimic the problem because current benchmark implements the transpose of a matrix and legion optimizes the transposition of  data by allocating the input and output data blocks already transposed so that no data movement actually takes place only the transpose of a the blocks in local memory.
\item resilience: the runtime must be stable enough to handle hardware failures and take into account hints from the "driven program" which will tell the system the liveness of its k-inputs and output data (preciousness) and upon a failure of up to $m$ of the $k$ inputs compute a partial answer without an error condition. Legion runtime does not currently employ a distributed state machine mechanism, which makes fault tolerance difficult to implement. Right now, it is possible for a failed program to run until SLURM terminates it due to timeout.  
\item Maintainability/ Extensibility: tasks and runtime should be separated by a well defined interface so that functionality can be extended with very little knowledge. This is possible for Regent, plus it provides the mapper interface to add specific details about the scheduling, but it should only need to be written once and I do not know at this point if this is possible.
\item Performance Portability/ Extensibility: the runtime should be portable to different platforms and specifically should be able to run in a upgraded hardware (i.e with added memory per node, better network, more nodes ...) and take advantadge of the new available resources with minimal changes to the code. This should be ok too.
\end{itemize}

NOTE: Most of the information compiled in this section comes from the memo~\cite{RegentEval}, where more detailed analysis, justification as well as information on how to run the tests can be found.
